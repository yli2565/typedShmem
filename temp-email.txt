Hi, professor, I totally agree we can do much more behavior cloning in the fall semester.

It has been a couple of week from our last meeting, and here's what I'm doing since then

I'm packing up my code into modules that can couple with future BHuman code. 
Currently I've done: 
    A new communication module (not finished yet)
    Robot side FSM logic finished
    GameController FSM logic finished

    Rewrite the code that are written in a hurry

    Make LogReader use multi-processing in a better way (But python is still kind of slow for byte operation)

Another things is that , the speed of SimRObot env drop drastically when the number of robot increase, >=
4 robots will make the speed inpracticall to be used for training. Unfortunate, this problem have no easy fix.  our code is using pettingzoo's parellel env, which requires all agents to do synchorized step, while
SimRobot's agents update in an aync way. Changing to  Agent Environment Cycle  (AEC) env would help, but would make the implementation quite hard

And these days I've tried to train agents will ability to select skills among BHuman's skills but doesn't succeed
I didn't know if in France Will change the implementation, but from the code he gives me, we have a small action space. 
After reading some papers and talking with Abhinav, I think we should either train subtask + task planner (Or not task planner in Abhinav's case) or use action masking (probably with a trained mask)
I think when you mention "a network that predict probability of goal" would it be used to do somekind of action masking? Or just simply used a observation input?


For my personal plan, I decide to delay my application and find a direction of interest before applying to
graduate school. So, I might have another year to do some research in UW_Madison with my OPT.

I don't have a deep understanding on exploration, but I think, our robot can work at a relatively high frequency and the "shaking behavior" will make the learning difficult. 

What I witness, is that since our robot works at a relatively high frequency, during training, the action it takes cannot represent it's action in the real world.
For example, shaking left and right will result in going forward, but when we sample transitions, it will get a single go-left or go-right action and thinks that contribute to the 
resulting reward. I think, even in exploration, in similar state space, the action should not be too random. 

So, your paper shows we can define subtask the is learned


Yes, I will apply in 2025. For the time being, I'm sure doing research interest me much more than working in a company, but
One of the main concern is that, if I apply now, I might choose a direction that that I'm not interested in and will work on for the following two or even four years(If I apply for doc).
I still planned to graduate in 2025 Summer, but I will stay in UW for another year in some way, probability OPT. I also want your advise on if there's any lab members working in this lab after they graduate? 

I wanted to inform you that I plan to apply in 2025. Currently, I found doing research more interesting than working in a company. However, I have some concerns about choosing a research group without knowing my own interest, as I may end up working in an area Iâ€™m not fully passionate about for the next two to four years, especially if I pursue a Ph.D.

In a short term, I still plan to graduate in Summer 2025, but I intend to stay at UW for another year, possibly through OPT. I would appreciate your advice on whether any lab members have continued working in this lab after graduation.